# -*- coding: utf-8 -*-
"""BART-CNNDM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1c54hkTC9cUcQ5CzqRn2XD8IeQzw1rGtk
"""

# set up, install all required librarys and packages, add !,means it will excucuate as shell commnad, not for python
#google drive file
! pip install gdown
#sequence package
! pip install fairseq
#xml parser,for rouge computing
! apt-get install libxml-parser-perl

# download the cnn and dm data
# what if you don't add ! what is the difference? (! stands for shell command, if not, it is treated as python command)

# download daily mail stories data as a zipped file
# use gdown  0BwmD_VLjROrfM1BxdkxVaTY2bWs as dm ,with file extension tgz
! gdown 0BwmD_VLjROrfM1BxdkxVaTY2bWs -O dm.tgz
# download cnn mail stories data as a zipped file similarly, for the second one
! gdown 0BwmD_VLjROrfTHk4NFg2SndKcjQ -O cnn.tgz
# extract data to directories, tar 是打包器
! tar -zxf dm.tgz
! tar -zxf cnn.tgz
! ls .

# preprocess the data as specified by BART
# clone the repo for preprocessing the cnndm dataset as required in BART

# clone
! git clone https://github.com/artmatsak/cnn-dailymail.git
# move all things in the repo to current directory
! mv cnn-dailymail/* .
# do preprocess 
! python make_datafiles.py cnn/stories dailymail/stories > tmp.out
! ls .

# clone
! git clone https://github.com/artmatsak/cnn-dailymail.git

! mv cnn-dailymail/* .

# do preprocess 
! python make_datafiles.py cnn/stories dailymail/stories

# this block can be skipped if you upload your prediction file 

# load the pretrained bart model from pytorch hub 
import torch
# load
bart = torch.hub.load("pytorch/fairseq", "bart.large.cnn")
# set to evaluation mode, dropout disabled 
bart = bart.eval()
# move to gpu and set to half precision calculation，make it as 16 digit accuracy
bart = bart.cuda().half()

# this block can be skipped if you upload your prediction file 

# reference from  https://github.com/facebookresearch/fairseq/blob/main/examples/bart/summarize.py

# original function BART used to gernerate summary 
eval_kwargs = dict(beam=4, lenpen=2.0, max_len_b=140, min_len=55, no_repeat_ngram_size=3)
@torch.no_grad()
#n_obs number of summarization, default value is none,**eval_kwargs= beam=4, lenpen=2.0, max_len_b=140, min_len=55, no_repeat_ngram_size=3
def generate(bart, infile, outfile, bsz=32, n_obs=None, **eval_kwargs):
    count = 1
    #count : number of total lines you have already processed
    with open(infile) as source, open(outfile, "w") as fout:
        sline = source.readline().strip()# read one line
        slines = [sline]# store the line into array
        for sline in source:
            if n_obs is not None and count > n_obs:
                break
            if count % bsz == 0:
                print('Working to {}'.format(count), end = '\n')
                # slines:32 lines you have read
                hypotheses_batch = bart.sample(slines, **eval_kwargs)
                for hypothesis in hypotheses_batch:
                  # hypothesis : summarized article
                    fout.write(hypothesis + "\n")
                    fout.flush()
                slines = []

            slines.append(sline.strip())
            count += 1
            

        if slines != []:
            hypotheses_batch = bart.sample(slines, **eval_kwargs)
            for hypothesis in hypotheses_batch:
                fout.write(hypothesis + "\n")
                fout.flush()

# gernerate all summary for test set and save into test.hypo in cnn_dm directory.
generate(bart, infile = 'cnn_dm/test.source', outfile="cnn_dm/test.hypo", n_obs = None, **eval_kwargs)

# install file2rouge 

# first install pyrouge
! pip install -U git+https://github.com/pltrdy/pyrouge

# then install file2rouge
# because it involves change directory command in bash
# we create a bash file and execute it. 
# this is a trick in python notebook to run bash commands 
commands = '''git clone https://github.com/pltrdy/files2rouge.git     
cd files2rouge
python setup_rouge.py
python setup.py install
'''

with open('install_rouge.sh', 'w') as f:
    f.write(commands)

! bash ./install_rouge.sh

commands='''cd sample_data
ls
'''

fh = open('jiaoben.sh','w')
fh.write(commands)
fh.close()

! bash jiaoben.sh

# calculate the rouge score as stated in bart
# need to tokenized everything use stanford corenlp utils 

# download the utils
! wget https://repo1.maven.org/maven2/edu/stanford/nlp/stanford-corenlp/3.7.0/stanford-corenlp-3.7.0.jar -O corenlp.jar

# tokenize is a set of bash commands and use the same trick as above 
commands = '''export CLASSPATH=./corenlp.jar
cat cnn_dm/test.hypo | java edu.stanford.nlp.process.PTBTokenizer -ioFileList -preserveLines > test.hypo.tokenized
cat cnn_dm/test.target | java edu.stanford.nlp.process.PTBTokenizer -ioFileList -preserveLines > test.hypo.target
'''

with open('tokenize.sh', 'w') as f:
    f.write(commands)
# execute
! bash tokenize.sh

# calculate the rouge score
! files2rouge --ignore_empty_reference test.hypo.tokenized test.hypo.target


! files2rouge --help